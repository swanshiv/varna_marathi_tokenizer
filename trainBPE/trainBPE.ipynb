{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "BPE Tokenizer Training Script\n",
        "Trains a Byte-Level BPE tokenizer on Marathi corpus.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fLHcGnjDeoRq",
        "outputId": "ccb096a2-84d9-4c2b-eab8-7b9cd70c3902"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nBPE Tokenizer Training Script\\nTrains a Byte-Level BPE tokenizer on Marathi corpus.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from collections import defaultdict\n",
        "from typing import List, Tuple, Dict\n"
      ],
      "metadata": {
        "id": "zYZVcApweqWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_corpus(file_path: str) -> str:\n",
        "    \"\"\"Read the corpus file.\"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        return f.read()\n"
      ],
      "metadata": {
        "id": "e42T3VcAersl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_bytes(text: str) -> List[int]:\n",
        "    \"\"\"Convert text to list of UTF-8 byte values (0-255).\"\"\"\n",
        "    return list(text.encode('utf-8'))\n"
      ],
      "metadata": {
        "id": "QRzOnDVvetS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_word_frequencies(text: str) -> Dict[Tuple[int, ...], int]:\n",
        "    \"\"\"\n",
        "    Split text into words and count frequencies.\n",
        "    For BPE, we treat each word as a sequence of bytes.\n",
        "    \"\"\"\n",
        "    words = text.split()\n",
        "    word_freqs = defaultdict(int)\n",
        "\n",
        "    for word in words:\n",
        "        if word.strip():  # Skip empty words\n",
        "            byte_word = tuple(text_to_bytes(word))\n",
        "            word_freqs[byte_word] += 1\n",
        "\n",
        "    return dict(word_freqs)\n"
      ],
      "metadata": {
        "id": "v3YWQKCQevls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stats(vocab: Dict[Tuple[int, ...], int]) -> Dict[Tuple[int, int], int]:\n",
        "    \"\"\"Count frequency of adjacent pairs in the vocabulary.\"\"\"\n",
        "    pairs = defaultdict(int)\n",
        "\n",
        "    for word, freq in vocab.items():\n",
        "        for i in range(len(word) - 1):\n",
        "            pair = (word[i], word[i + 1])\n",
        "            pairs[pair] += freq\n",
        "\n",
        "    return dict(pairs)\n"
      ],
      "metadata": {
        "id": "e6eKrwXXe1M5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_vocab(pair: Tuple[int, int], vocab: Dict[Tuple[int, ...], int], new_token_id: int) -> Dict[Tuple[int, ...], int]:\n",
        "    \"\"\"Merge the most frequent pair in the vocabulary.\"\"\"\n",
        "    new_vocab = {}\n",
        "    bigram = pair[0], pair[1]\n",
        "\n",
        "    for word in vocab:\n",
        "        new_word = []\n",
        "        i = 0\n",
        "        while i < len(word):\n",
        "            if i < len(word) - 1 and word[i] == bigram[0] and word[i + 1] == bigram[1]:\n",
        "                # Merge: replace pair with new token ID\n",
        "                new_word.append(new_token_id)\n",
        "                i += 2\n",
        "            else:\n",
        "                new_word.append(word[i])\n",
        "                i += 1\n",
        "        new_vocab[tuple(new_word)] = vocab[word]\n",
        "\n",
        "    return new_vocab\n"
      ],
      "metadata": {
        "id": "gfNLPuNAe3nm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_tokens_after_merges(vocab: Dict[Tuple[int, ...], int]) -> int:\n",
        "    \"\"\"\n",
        "    Count total tokens after applying all merges.\n",
        "    Each element in the word tuple is a token ID.\n",
        "    \"\"\"\n",
        "    total = 0\n",
        "\n",
        "    for word, freq in vocab.items():\n",
        "        # Count tokens: each element in word is a token ID\n",
        "        token_count = len(word)\n",
        "        total += token_count * freq\n",
        "\n",
        "    return total\n"
      ],
      "metadata": {
        "id": "pD71y8tge53o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_bpe(\n",
        "    corpus_path: str,\n",
        "    target_vocab_size: int = 4999\n",
        ") -> Tuple[List[Tuple[int, int]], Dict[int, Tuple[int, ...]], int, float]:\n",
        "    \"\"\"\n",
        "    Train BPE tokenizer.\n",
        "\n",
        "    Returns:\n",
        "        merges: List of merge rules [(byte1, byte2), ...]\n",
        "        vocab_map: Dictionary mapping token_id to byte sequence\n",
        "        final_vocab_size: Final vocabulary size\n",
        "        compression_ratio: Compression ratio achieved\n",
        "    \"\"\"\n",
        "    print(\"Reading corpus...\")\n",
        "    text = read_corpus(corpus_path)\n",
        "    original_char_count = len(text)\n",
        "    print(f\"Corpus loaded: {original_char_count:,} characters\")\n",
        "\n",
        "    print(\"Converting text to bytes and computing word frequencies...\")\n",
        "    word_freqs = get_word_frequencies(text)\n",
        "    print(f\"Unique words: {len(word_freqs):,}\")\n",
        "\n",
        "    # Initialize vocabulary: each word is a sequence of bytes\n",
        "    vocab = word_freqs.copy()\n",
        "\n",
        "    # Track merges\n",
        "    merges = []\n",
        "\n",
        "    # Track vocabulary mapping: token_id -> byte sequence\n",
        "    # Start with 256 base byte tokens\n",
        "    vocab_map = {}\n",
        "    for i in range(256):\n",
        "        vocab_map[i] = (i,)\n",
        "\n",
        "    token_id = 256  # Next token ID for merged tokens\n",
        "\n",
        "    print(f\"\\nStarting BPE training...\")\n",
        "    print(f\"Target vocabulary size: {target_vocab_size}\")\n",
        "    print(f\"Initial vocabulary size: {len(vocab_map)}\")\n",
        "\n",
        "    iteration = 0\n",
        "\n",
        "    while len(vocab_map) < target_vocab_size:\n",
        "        pairs = get_stats(vocab)\n",
        "        if not pairs:\n",
        "            print(\"No more pairs to merge!\")\n",
        "            break\n",
        "\n",
        "        # Find most frequent pair\n",
        "        best_pair = max(pairs, key=pairs.get)\n",
        "        best_freq = pairs[best_pair]\n",
        "\n",
        "        if best_freq < 2:  # Stop if no frequent pairs\n",
        "            break\n",
        "\n",
        "        # Add merge rule\n",
        "        merges.append(best_pair)\n",
        "\n",
        "        # Add to vocabulary mapping BEFORE merging\n",
        "        # The merged token is the concatenation of the two tokens\n",
        "        token1_seq = vocab_map[best_pair[0]]\n",
        "        token2_seq = vocab_map[best_pair[1]]\n",
        "        merged_seq = token1_seq + token2_seq\n",
        "        vocab_map[token_id] = merged_seq\n",
        "\n",
        "        # Merge the pair in vocabulary (replace pair with new token ID)\n",
        "        vocab = merge_vocab(best_pair, vocab, token_id)\n",
        "        token_id += 1\n",
        "\n",
        "        iteration += 1\n",
        "        if iteration % 100 == 0:\n",
        "            print(f\"Iteration {iteration}: Vocabulary size = {len(vocab_map)}, \"\n",
        "                  f\"Most frequent pair = {best_pair} (freq: {best_freq:,})\")\n",
        "\n",
        "        if len(vocab_map) >= target_vocab_size:\n",
        "            break\n",
        "\n",
        "    # Count total tokens after encoding\n",
        "    # Each element in vocab keys is a token ID\n",
        "    total_tokens = count_tokens_after_merges(vocab)\n",
        "\n",
        "    # Calculate compression ratio\n",
        "    compression_ratio = original_char_count / total_tokens if total_tokens > 0 else 0\n",
        "\n",
        "    final_vocab_size = len(vocab_map)\n",
        "\n",
        "    print(f\"\\nTraining completed!\")\n",
        "    print(f\"Total iterations: {iteration}\")\n",
        "    print(f\"Final vocabulary size: {final_vocab_size}\")\n",
        "    print(f\"Total merges: {len(merges)}\")\n",
        "    print(f\"Original character count: {original_char_count:,}\")\n",
        "    print(f\"Total tokens after encoding: {total_tokens:,}\")\n",
        "    print(f\"Compression ratio: {compression_ratio:.2f}x\")\n",
        "\n",
        "    return merges, vocab_map, final_vocab_size, compression_ratio\n"
      ],
      "metadata": {
        "id": "Iki_BzJ7e_7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(\n",
        "    merges: List[Tuple[int, int]],\n",
        "    vocab_map: Dict[int, Tuple[int, ...]],\n",
        "    vocab_size: int,\n",
        "    compression_ratio: float,\n",
        "    output_path: str\n",
        "):\n",
        "    \"\"\"Save the trained model to JSON file.\"\"\"\n",
        "    # Convert vocab_map to serializable format\n",
        "    vocab_serializable = {\n",
        "        str(k): list(v) for k, v in vocab_map.items()\n",
        "    }\n",
        "\n",
        "    model_data = {\n",
        "        \"merges\": merges,\n",
        "        \"vocab\": vocab_serializable,\n",
        "        \"vocab_size\": vocab_size,\n",
        "        \"compression_ratio\": compression_ratio,\n",
        "        \"metadata\": {\n",
        "            \"algorithm\": \"Byte-Level BPE\",\n",
        "            \"base_tokens\": 256,\n",
        "            \"num_merges\": len(merges)\n",
        "        }\n",
        "    }\n",
        "\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(model_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"\\nModel saved to: {output_path}\")\n"
      ],
      "metadata": {
        "id": "NSXVGuGifCem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Main training function.\"\"\"\n",
        "    # Paths\n",
        "    corpus_path = '/content/drive/MyDrive/MarathiBPE/data/corpus_small.txt'\n",
        "    output_path = '/content/drive/MyDrive/MarathiBPE/backend/merges.json'\n",
        "\n",
        "    # Ensure output directory exists\n",
        "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "\n",
        "    # Check if corpus exists\n",
        "    if not os.path.exists(corpus_path):\n",
        "        print(f\"Error: Corpus file not found at {corpus_path}\")\n",
        "        return\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Marathi BPE Tokenizer Training\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Train BPE\n",
        "    merges, vocab_map, vocab_size, compression_ratio = train_bpe(\n",
        "        corpus_path=corpus_path,\n",
        "        target_vocab_size=4999\n",
        "    )\n",
        "\n",
        "    # Validate constraints\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"Validation Results:\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    if vocab_size < 5000:\n",
        "        print(f\"✓ Vocabulary size constraint met: {vocab_size} < 5000\")\n",
        "    else:\n",
        "        print(f\"✗ Vocabulary size constraint NOT met: {vocab_size} >= 5000\")\n",
        "\n",
        "    if compression_ratio >= 3.2:\n",
        "        print(f\"✓ Compression ratio constraint met: {compression_ratio:.2f}x >= 3.2x\")\n",
        "    else:\n",
        "        print(f\"✗ Compression ratio constraint NOT met: {compression_ratio:.2f}x < 3.2x\")\n",
        "\n",
        "    # Save model\n",
        "    save_model(merges, vocab_map, vocab_size, compression_ratio, output_path)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"Training completed successfully!\")\n",
        "    print(\"=\" * 60)\n"
      ],
      "metadata": {
        "id": "dUYEoDJufFaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTsWBsyVdDiK",
        "outputId": "e6082dc0-519d-4cc9-a892-4786c4b193a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Marathi BPE Tokenizer Training\n",
            "============================================================\n",
            "Reading corpus...\n",
            "Corpus loaded: 7,321,896 characters\n",
            "Converting text to bytes and computing word frequencies...\n",
            "Unique words: 118,738\n",
            "\n",
            "Starting BPE training...\n",
            "Target vocabulary size: 4999\n",
            "Initial vocabulary size: 256\n",
            "Iteration 100: Vocabulary size = 356, Most frequent pair = (259, 160) (freq: 11,706)\n",
            "Iteration 200: Vocabulary size = 456, Most frequent pair = (264, 285) (freq: 5,258)\n",
            "Iteration 300: Vocabulary size = 556, Most frequent pair = (278, 294) (freq: 3,034)\n",
            "Iteration 400: Vocabulary size = 656, Most frequent pair = (305, 329) (freq: 2,166)\n",
            "Iteration 500: Vocabulary size = 756, Most frequent pair = (270, 298) (freq: 1,638)\n",
            "Iteration 600: Vocabulary size = 856, Most frequent pair = (282, 493) (freq: 1,308)\n",
            "Iteration 700: Vocabulary size = 956, Most frequent pair = (318, 806) (freq: 1,128)\n",
            "Iteration 800: Vocabulary size = 1056, Most frequent pair = (268, 499) (freq: 957)\n",
            "Iteration 900: Vocabulary size = 1156, Most frequent pair = (270, 934) (freq: 821)\n",
            "Iteration 1000: Vocabulary size = 1256, Most frequent pair = (259, 165) (freq: 712)\n",
            "Iteration 1100: Vocabulary size = 1356, Most frequent pair = (378, 184) (freq: 640)\n",
            "Iteration 1200: Vocabulary size = 1456, Most frequent pair = (589, 295) (freq: 569)\n",
            "Iteration 1300: Vocabulary size = 1556, Most frequent pair = (276, 268) (freq: 523)\n",
            "Iteration 1400: Vocabulary size = 1656, Most frequent pair = (283, 536) (freq: 470)\n",
            "Iteration 1500: Vocabulary size = 1756, Most frequent pair = (519, 587) (freq: 424)\n",
            "Iteration 1600: Vocabulary size = 1856, Most frequent pair = (1524, 281) (freq: 393)\n",
            "Iteration 1700: Vocabulary size = 1956, Most frequent pair = (289, 267) (freq: 360)\n",
            "Iteration 1800: Vocabulary size = 2056, Most frequent pair = (296, 285) (freq: 334)\n",
            "Iteration 1900: Vocabulary size = 2156, Most frequent pair = (522, 413) (freq: 310)\n",
            "Iteration 2000: Vocabulary size = 2256, Most frequent pair = (307, 366) (freq: 290)\n",
            "Iteration 2100: Vocabulary size = 2356, Most frequent pair = (336, 307) (freq: 269)\n",
            "Iteration 2200: Vocabulary size = 2456, Most frequent pair = (388, 798) (freq: 253)\n",
            "Iteration 2300: Vocabulary size = 2556, Most frequent pair = (276, 353) (freq: 235)\n",
            "Iteration 2400: Vocabulary size = 2656, Most frequent pair = (294, 337) (freq: 221)\n",
            "Iteration 2500: Vocabulary size = 2756, Most frequent pair = (267, 284) (freq: 210)\n",
            "Iteration 2600: Vocabulary size = 2856, Most frequent pair = (484, 388) (freq: 199)\n",
            "Iteration 2700: Vocabulary size = 2956, Most frequent pair = (315, 344) (freq: 189)\n",
            "Iteration 2800: Vocabulary size = 3056, Most frequent pair = (270, 1432) (freq: 180)\n",
            "Iteration 2900: Vocabulary size = 3156, Most frequent pair = (893, 263) (freq: 171)\n",
            "Iteration 3000: Vocabulary size = 3256, Most frequent pair = (427, 299) (freq: 164)\n",
            "Iteration 3100: Vocabulary size = 3356, Most frequent pair = (290, 309) (freq: 157)\n",
            "Iteration 3200: Vocabulary size = 3456, Most frequent pair = (343, 283) (freq: 150)\n",
            "Iteration 3300: Vocabulary size = 3556, Most frequent pair = (281, 2218) (freq: 143)\n",
            "Iteration 3400: Vocabulary size = 3656, Most frequent pair = (1452, 266) (freq: 137)\n",
            "Iteration 3500: Vocabulary size = 3756, Most frequent pair = (273, 1553) (freq: 131)\n",
            "Iteration 3600: Vocabulary size = 3856, Most frequent pair = (264, 308) (freq: 126)\n",
            "Iteration 3700: Vocabulary size = 3956, Most frequent pair = (506, 479) (freq: 122)\n",
            "Iteration 3800: Vocabulary size = 4056, Most frequent pair = (1251, 810) (freq: 117)\n",
            "Iteration 3900: Vocabulary size = 4156, Most frequent pair = (387, 1879) (freq: 113)\n",
            "Iteration 4000: Vocabulary size = 4256, Most frequent pair = (398, 2799) (freq: 110)\n",
            "Iteration 4100: Vocabulary size = 4356, Most frequent pair = (559, 1199) (freq: 106)\n",
            "Iteration 4200: Vocabulary size = 4456, Most frequent pair = (264, 555) (freq: 102)\n",
            "Iteration 4300: Vocabulary size = 4556, Most frequent pair = (1296, 3927) (freq: 99)\n",
            "Iteration 4400: Vocabulary size = 4656, Most frequent pair = (439, 428) (freq: 95)\n",
            "Iteration 4500: Vocabulary size = 4756, Most frequent pair = (267, 297) (freq: 92)\n",
            "Iteration 4600: Vocabulary size = 4856, Most frequent pair = (4456, 318) (freq: 89)\n",
            "Iteration 4700: Vocabulary size = 4956, Most frequent pair = (568, 262) (freq: 86)\n",
            "\n",
            "Training completed!\n",
            "Total iterations: 4743\n",
            "Final vocabulary size: 4999\n",
            "Total merges: 4743\n",
            "Original character count: 7,321,896\n",
            "Total tokens after encoding: 1,858,048\n",
            "Compression ratio: 3.94x\n",
            "\n",
            "============================================================\n",
            "Validation Results:\n",
            "============================================================\n",
            "✓ Vocabulary size constraint met: 4999 < 5000\n",
            "✓ Compression ratio constraint met: 3.94x >= 3.2x\n",
            "\n",
            "Model saved to: /content/drive/MyDrive/MarathiBPE/backend/merges.json\n",
            "\n",
            "============================================================\n",
            "Training completed successfully!\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}